# 1.集成学习
在机器学习中,集成学习(Ensemble Learning)是一种将多个基学习器结合起来形成更强大的学习器的方法。
1. Bagging:通过自助采样从原始数据集抽取多个训练集,分别训练基学习器,最后通过投票或平均等方式集成。典型代表是随机森林。
2. Boosting:通过迭代的方式训练一系列基学习器,每次迭代时对前一轮分类错误的样本增加权重。典型代表有AdaBoost、GBDT等。
3. Blending:将数据集分成两部分,用第一部分数据训练多个基学习器,再用第二部分数据训练一个组合器来融合这些基学习器。
# 2.正则化
机器学习常常会遇到“过拟合”的问题。也就是说，模型在训练数据上表现得非常好，但在测试数据上却一塌糊涂。为了防止模型“记住”数据而不是“学习”规律，我们通常会在训练时加入一种约束，让模型不要太复杂——这就是正则化（Regularization）
 ## L1 正则化
其惩罚项是参数的绝对值之和：
<img width="399" height="104" alt="Image" src="https://github.com/user-attachments/assets/88a0e7ce-16d8-47eb-85a7-89586737a0b7" />
参数 wi​ 越大，惩罚越强。L1 的一个特别之处在于：它会让某些参数直接变成 0。也就是说，它不仅能让模型更简单，还能起到特征选择的作用——把不重要的特征直接“删除”掉。
## L2 正则化
其惩罚项是参数平方和：
<img width="414" height="111" alt="Image" src="https://github.com/user-attachments/assets/95846661-2c9c-4427-8a54-784a456c0a45" />
 这里的惩罚项让模型在训练时“更平滑”，即不让某个参数特别大，也不会让某个参数直接变成 0。因此，L2 正则化的效果是让所有权重都“更小”，但仍然保留在模型中。