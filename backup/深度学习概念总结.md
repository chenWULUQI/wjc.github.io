# 1.集成学习
在机器学习中,集成学习(Ensemble Learning)是一种将多个基学习器结合起来形成更强大的学习器的方法。
1. Bagging:通过自助采样从原始数据集抽取多个训练集,分别训练基学习器,最后通过投票或平均等方式集成。典型代表是随机森林。
2. Boosting:通过迭代的方式训练一系列基学习器,每次迭代时对前一轮分类错误的样本增加权重。典型代表有AdaBoost、GBDT等。
3. Blending:将数据集分成两部分,用第一部分数据训练多个基学习器,再用第二部分数据训练一个组合器来融合这些基学习器。
# 2.正则化
机器学习常常会遇到“过拟合”的问题。也就是说，模型在训练数据上表现得非常好，但在测试数据上却一塌糊涂。为了防止模型“记住”数据而不是“学习”规律，我们通常会在训练时加入一种约束，让模型不要太复杂——这就是正则化（Regularization）
 ## L1 正则化
其惩罚项是参数的绝对值之和：
<img width="399" height="104" alt="Image" src="https://github.com/user-attachments/assets/88a0e7ce-16d8-47eb-85a7-89586737a0b7" />
参数 wi​ 越大，惩罚越强。L1 的一个特别之处在于：它会让某些参数直接变成 0。也就是说，它不仅能让模型更简单，还能起到特征选择的作用——把不重要的特征直接“删除”掉。
## L2 正则化
其惩罚项是参数平方和：
<img width="414" height="111" alt="Image" src="https://github.com/user-attachments/assets/95846661-2c9c-4427-8a54-784a456c0a45" />
 这里的惩罚项让模型在训练时“更平滑”，即不让某个参数特别大，也不会让某个参数直接变成 0。因此，L2 正则化的效果是让所有权重都“更小”，但仍然保留在模型中。
# 3.常见的几种激活函数
## 1.Sigmoid函数
数学表达式：
<img width="160" height="68" alt="Image" src="https://github.com/user-attachments/assets/d874218a-6a65-44cf-b6a3-26fc680ac530" />
导数表达式为：
<img width="154" height="32" alt="Image" src="https://github.com/user-attachments/assets/ec36fbf8-df6f-44e3-afea-9e315101d12d" />
图像：
<img width="566" height="422" alt="Image" src="https://github.com/user-attachments/assets/07447ca6-91bb-420a-b247-3893c5f5541f" />
### 用处
Sigmoid 函数的输出范围是 0 到 1。非常适合作为模型的输出函数用于输出一个0~1范围内的概率值，比如用于表示二分类的类别或者用于表示置信度。且其梯度平滑，便于求导，也防止模型训练过程中出现突变的梯度。
### 缺点
容易造成梯度消失。我们从导函数图像中了解到sigmoid的导数都是小于0.25的，那么在进行反向传播的时候，梯度相乘结果会慢慢的趋向于0。这样几乎就没有梯度信号通过神经元传递到前面层的梯度更新中，因此这时前面层的权值几乎没有更新，这就叫梯度消失。除此之外，为了防止饱和，必须对于权重矩阵的初始化特别留意。如果初始化权重过大，可能很多神经元得到一个比较小的梯度，致使神经元不能很好的更新权重提前饱和，神经网络就几乎不学习。

## 2.Tanh函数
数学表达式：
<img width="155" height="67" alt="Image" src="https://github.com/user-attachments/assets/7639696e-ec92-4097-955c-567a077e03cb" />
图像：
<img width="627" height="446" alt="Image" src="https://github.com/user-attachments/assets/771e93e9-b5cb-4c38-ad42-8ec190b2ae78" />
### 用处
tanh 的输出间隔为 1，并且整个函数以 0 为中心，比 sigmoid 函数更好；在 tanh 图中，负输入将被强映射为负，而零输入被映射为接近零。


## 3.ReLU函数
数学表达式：
<img width="165" height="43" alt="Image" src="https://github.com/user-attachments/assets/ff0276cc-e1d8-4e0b-927b-7cad3567bf67" />
图像：
<img width="602" height="452" alt="Image" src="https://github.com/user-attachments/assets/6877b732-67e2-4ea9-86f8-82cef625300b" />
### 用处
ReLU解决了梯度消失的问题，当输入值为正时，神经元不会饱和。由于ReLU线性、非饱和的性质，在SGD中能够快速收敛。计算复杂度低，不需要进行指数运算。
### 缺点
与Sigmoid一样，其输出不是以0为中心的。Dead ReLU 问题。当输入为负时，梯度为0。这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应，导致相应参数永远不会被更新。

## 4.Leaky Relu函数
数学表达式：
<img width="168" height="40" alt="Image" src="https://github.com/user-attachments/assets/52d0f5b9-0536-4676-ab41-85c0832ccad7" />
图像：
<img width="564" height="443" alt="Image" src="https://github.com/user-attachments/assets/7dbb69b1-b462-41f5-ac98-03ade0a8c00e" />
### 用处
解决了ReLU输入值为负时神经元出现的死亡的问题
### 缺点
函数中的α，需要通过先验知识人工赋值（一般设为0.01）。有些近似线性，导致在复杂分类中效果不好。

## 5.Softmax函数
数学表达式：
<img width="191" height="59" alt="Image" src="https://github.com/user-attachments/assets/5c9a6a2f-1177-4a43-a0ec-81b8fe64eda7" />
图像：
<img width="412" height="298" alt="Image" src="https://github.com/user-attachments/assets/387e15f3-8578-4db8-aed0-6f3f803a5033" />
### 用处
Softmax函数常在神经网络输出层充当激活函数，将输出层的值通过激活函数映射到0-1区间，将神经元输出构造成概率分布，用于多分类问题中，Softmax激活函数映射值越大，则真实类别可能性越大。
