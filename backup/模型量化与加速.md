# 一，模型量化
1.模型量化是将浮点数值转化为定点数值，同时尽可能减少计算精度损失的方法。具体而言，模型量化是一种压缩网络参数的方式，它将神经网络的参数（weight）、特征图（activation）等原本用浮点表示的量值换用定点（整型）表示，在计算过程中，再将定点数据反量化回浮点数据，得到结果。
fp16格式存储数据过程：
<img width="740" height="349" alt="Image" src="https://github.com/user-attachments/assets/47e889d9-280d-4a7f-aaf3-7b6bb54e9658" />
共分为五步：1.转成二进制。2.确定符号位。3.计算阶码（即指数部分的偏移量）。4.计算尾数（即小数部分）5.拼接在一起表示。
以6.5为例：
1.转成二进制。
6.5（10）​=110.1（2）​
2.确定符号位。
为正，即第一位为0
3..计算阶码。
<img width="164" height="40" alt="Image" src="https://github.com/user-attachments/assets/55e2e4df-5a45-4b8d-bedb-78086bec94b0" />
<img width="464" height="148" alt="Image" src="https://github.com/user-attachments/assets/bbf95788-4d40-4931-9525-f84c6916df9e" />
4.计算尾数（即小数部分）。
<img width="518" height="155" alt="Image" src="https://github.com/user-attachments/assets/7efb6f92-afd9-4191-bd62-353c33d484d4" />
5.拼接在一起表示为0 10001 1010000000

# 二，KV Cache 
https://zhuanlan.zhihu.com/p/662498827
在 Transformer 模型中，KV Cache（Key-Value Cache，键值缓存） 指的是自回归推理过程中缓存的Key 和Value，以避免重复计算。它主要用于自注意力机制（Self-Attention），在解码（Decoder） 过程中缓存之前计算过的 Key（键）和 Value（值），从而加速生成。在没有KV Cache的情况下，每次生成一个新的 token 时，模型都需要重新计算所有 token 的 Query（查询）、Key（键）、Value（值），导致计算冗余。而使用 KV Cache 后，只需要计算新 token 的 Query，并与已缓存的 KV 进行注意力计算，大幅提高推理速度。
## 2.1训练时需要 KV Cache 吗？还是只有推理时需要？
训练时 不需要 KV Cache，通常不会使用它。推理时 需要 KV Cache，以加速生成，减少计算冗余。但更深入地理解 KV Cache 的作用，需要从 Transformer 训练和推理的计算模式入手。
### 2.1.1 训练（Training）
在 Transformer 训练阶段，所有 token 是并行计算的，因此不需要 KV Cache：输入是完整的序列，可以一次性计算所有 Q, K, V，自注意力计算 QK^T 也可以一次性完成。计算复杂度 O(N²)：对于序列长度 N，计算 QK^T 需要 O(N²)。但 GPU 可以并行计算所有 token，因此在训练阶段这个计算量是可接受的。

### 2.1.2 推理（Inference）
推理（文本生成）采用 自回归（Auto-Regressive） 方式：每次生成一个新的 token，只能依赖之前生成的 token 计算新的 QK^T。计算复杂度 O(N²) → O(N)：无 KV Cache：每个 token 生成时都重新计算 K, V，计算复杂度 O(N²)。有 KV Cache：只计算当前 token 的 Q，并复用之前缓存的 K, V，计算复杂度降为 O(N)。

# 三，TensorRT
https://blog.csdn.net/qq_58158950/article/details/143061098
https://blog.csdn.net/IAMoldpan/article/details/117908232

# 四，大模型推理过程概述
https://zhuanlan.zhihu.com/p/25708488005
输入是“What color is the sky”，注意在大模型推理中输入也称为prompt
输出是“The sky is blue.“，在大模型推理中输出也称为completion
使用基于Transformer的大模型，从prompt生成completion的过程分为如下五个步骤：
第1步，将大模型的模型权重加载到GPU的显存中；第2步，在CPU上对prompt做tokenization（分词），并将token的张量表示（token_ids）从内存传输到GPU的显存中；第3步，让token_ids走一遍网络得到输出的第一个token，这一步只需要走一遍网络，也被称为prefill（预填充）阶段；第4步，将生成的token拼接到输入的token的结尾，并以此作为一个新的输入来生成下一个token。重复上述过程，直到生成一个终止符的token或者达到配置的最大生成长度。终止符token也被称为end-of-sequence (EOS) token，这一步需要反复走很多遍网络，也被称为decoding（解码）阶段，给到decoder的输入随着token的增加将会不断变长；第5步，将生成的所有token从GPU的显存传输回CPU，然后由CPU将生成的id映射回文本，这就是最终生成的completion。

